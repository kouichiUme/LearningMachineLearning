#sparkの起動
~/LML/spark-2.2.0-bin-hadoop2.7/bin$ ./spark-shell 


val lines = sc.textFile("../../timeline_20171022231916.csv")

lines.count();
lines.first();
lines.flatMap(line => line.split(","))


// NobyChatクラスを作成する
case class NobyChat( type3:Int,typeName:String,state:Int,comment:String,reply:String)

// fileからチャットデータを取り込み
import org.apache.spark.rdd.RDD
import java.text.SimpleDateFormat
val nobyRDD : RDD[(Int,String,Int,String,String)] = sc.textFile("../../timeline_clearhead.csv").
map { line => 
val dateFormat = new SimpleDateFormat("YYYY/MM/DD HH:mm")
 val words = line.split(",")
 // val hiduke 
 val type3 = words(2).replaceAll("\"", "")	.toInt
 val typeName = words(3)
 val state = words(4).replaceAll("\"", "")	.toInt
 val comment = words(5)
 val reply = words(6)
 ( type3 , typeName, state , comment, reply)
 }
 
 val nobyChatDF = nobyRDD.map { case ( type3, typeName, state, comment,reply) => 
 NobyChat (type3,typeName,state, comment,reply)}.toDF().cache()
 
 // 
 nobyChatDF.rdd.foreach( row => println(s"${row.get(0)}") )
 